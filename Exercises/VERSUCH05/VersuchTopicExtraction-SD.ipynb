{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Extraction in Newsfeeds\n",
    "* Autor: Prof. Dr. Johannes Maucher\n",
    "* Datum: 17.11.2015\n",
    "\n",
    "[Übersicht Ipython Notebooks im Data Mining Praktikum](Data Mining Praktikum.html)\n",
    "\n",
    "#Einführung\n",
    "## Lernziele:\n",
    "\n",
    "In diesem Versuch sollen Kenntnisse in folgenden Themen vermittelt werden:\n",
    "\n",
    "* __RSS Feeds:__ Struktur von RSS Feeds analysieren und parsen mit dem _Universal Feed Parser_. \n",
    "* __Dokument Analyse:__ Die Häufigkeit aller Worte in einem Dokument (Inhalt des RSS Feeds) zählen und in einem Array verwalten. \n",
    "* __Merkmalsextraktion:__ Bestimmung von Merkmalen (hier auch: __Topics__) (Allgemein spricht man von Merkmalen. Im Fall, dass die NNMF auf Dokumente angewandt wird, werden die Merkmale auch mit __Topics__ oder __Themen__ bezeichnet) mit der \\emph{Non Negative Matrix Factorization}.\n",
    "* __Zuordnung__: Wie setzen sich die Topics aus den Wörtern zusammen? Wie stark sind die gefundenen Topics in den Artikeln vertreten?\n",
    "* __Dokument Clustering:__ Mit der NNMF kann auch ein Clustering realisiert werden. Jeder Topic repräsentiert ein Cluster. Jedes Dokument wird dem Cluster zugeordnet, dessen Topic am stärksten in ihm vertreten ist. \n",
    "\n",
    "Sämtliche Verfahren und Algorithmen werden in Python implementiert.\n",
    "\n",
    "## Theorie zur Vorbereitung\n",
    "\n",
    "Stellen Sie sich vor Sie möchten in eine eigene Webseite die RSS Feeds einer Menge von Nachrichtenservern einbinden. Da die unterschiedlichen Server wahrscheinlich Artikel zu den gleichen Themen anbieten, werden die Inhalte einiger Artikel ähnlich sein. Mit der __Nicht Negativen Matrixfaktorisierung (NNMF)__ kann für eine große Menge von Dokumenten eine Menge von Themen (Topics) ermittelt werden, auf die sich die Dokumente beziehen. Damit ist es u.a. möglich\n",
    "* die Dokumente thematisch zu ordnen\n",
    "* zu jedem Thema nur ein Dokument anzuzeigen\n",
    "\n",
    "### Ähnlichkeiten bestimmen und relevante Merkmale extrahieren\n",
    "\n",
    "Eine Sammlung von Dokumenten - in diesem Versuch die Menge aller Nachrichten der angegebenen Feeds - kann in einer Artikel/Wort-Matrix repräsentiert werden. Jede Zeile dieser Matrix gehört zu einem Dokument. Für jedes Wort, das mindestens in einem der Dokumente vorkommt, ist eine Spalte vorgesehen. Das Matrixelement in Zeile $i$, Spalte $j$ beschreibt wie häufig das Wort in Spalte $j$ im zur Zeile $i$ gehörenden Dokument vorkommt.\n",
    "\n",
    "Unter der Annahme, dass Artikel umso ähnlicher sind, je mehr Worte in diesen gemeinsam vorkommen, kann auf der Grundlage dieser Matrix die Ähnlichkeit zwischen den Artikeln berechnet werden. Hierzu könnte die Matrix z.B. einfach einem _Hierarchischen Clustering_ übergeben werden. Das hierarchische Clustering weist jedoch im Fall einer großen Menge von zu vergleichenden Objekten zwei wesentliche Nachteile auf: Erstens ist die wiederholte Berechnung der Distanzen zwischen allen Artikeln/Clustern extrem rechenaufwendig, zweitens ist die Darstellung einer großen Anzahl von Objekten im Dendrogramm nicht mehr übersichtlich. \n",
    "\n",
    "Für das Auffinden von Assoziationen zwischen Dokumenten hat sich in den letzten Jahren die Methode der __Nicht-Negativen Matrix Faktorisierung (NNMF)__ etabliert. Mit dieser Methode kann eine Menge von wesentlichen Merkmalen berechnet werden, anhand derer sich die Dokumente clustern lassen, d.h. Dokumente des gleichen Clusters repräsentieren das gleiche Merkmal (Thema). Ein solches Merkmal wird durch eine Menge von Worten beschrieben, z.B. $\\{$ _Paris, terror, IS_ $\\}$  oder $\\{$_refugee, syria, border_ $\\}$. Neben der Merkmalsextraktion stellt die relativ geringe Komplexität einen weiteren Vorteil der NNMF dar. Durch die Darstellung der Artikel/Wort-Matrix als Produkt von 2 Faktormatrizen müssen deutlich weniger Einträge gespeichert werden.\n",
    "\n",
    "### Nicht Negative Matrixfaktorisierung: Die Idee\n",
    "\n",
    "Die Artikel/Wort-Matrix wird im Folgenden mit $A$ bezeichnet. Sie besitzt $r$ Zeilen und $c$ Spalten, wobei $r$ die Anzahl der Artikel und $c$ die Anzahl der relevanten Worte in der Menge aller Artikel ist. Durch Multiplikation der Matrix $A$ mit dem Vektor $v$ (_wordvec_: Vektor der alle relevanten Worte enthält) werden die Worte den Artikeln $a$ (_articletitles_: Vektor der alle Artikeltitel enthält) zugeordnet:\n",
    "\n",
    "$$\n",
    "a=A*v.\n",
    "$$\n",
    "\n",
    "Die Idee der NNMF besteht darin die Matrix $A$ als Produkt zweier Matrizen $W$ und $H$ darzustellen,\n",
    "\n",
    "$$\n",
    "A=W*H\n",
    "$$\n",
    "\n",
    "wobei alle Elemente in $W$ und $H$ größer oder gleich Null sein müssen. Die Matrixmultiplikation erfordert, dass die Anzahl der Zeilen $m$ in $H$ gleich der Anzahl der Spalten in $W$ sein muss. \n",
    "Durch die Faktorisierung der Matrix $A$ wird die Zuordnung der Wörter des Wortvektors $v$  zu den Artikeln des Vektors $a$ in zwei Stufen zerlegt. \n",
    "\n",
    "$$\n",
    "f = H*v\n",
    "$$\n",
    "$$\n",
    "a = W*f \n",
    "$$\n",
    "\n",
    "In der ersten Stufe werden durch die Multiplikation von $v$ mit der Matrix $H$ die Wörter einem sogenannten Merkmalsvektor $f$ mit $m$ Elementen zugewiesen. In der zweiten Stufe werden durch die Multiplikation des Merkmalsvektor $f$ mit der Matrix $W$ die einzelnen Merkmale den Artikeln in $a$ zugeordnet. Die Matrix $H$ definiert also aus welchen Wörtern die Merkmale gebildet werden. Sie wird deshalb __Merkmalsmatrix__ genannt. Die Matrix $W$ hingegen beschreibt mit welchem Gewicht die einzelnen Merkmale in den verschiedenen Artikeln auftreten. Sie wird deshalb __Gewichtungsmatrix__ genannt.\n",
    "\n",
    "Daraus folgt: Wenn eine Faktorisierung der Matrix $A$ gefunden wird, dann werden damit auch relevante Merkmale, also die Themen, definiert, hinsichtlich derer die Artikel effizient kategorisiert werden. Durch die Matrixfaktorisierung wird eine __Merkmalsextraktion__ realisiert. \n",
    "\n",
    "### Berechnung der Matrixfaktoren\n",
    "\n",
    "Für die Berechnung der Faktoren wurde in [Lee, Algortihms for Non-negative Matrix Factorisation](http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf) eine iterative Methode vorgestellt, die derzeit wohl am häufigsten angewandt wird und auch in dieser Übung implementiert werden soll. Der Algorithmus besteht aus folgenden Schritten:\n",
    "* Gebe die zu faktorisierende Matrix $A$ ein. $r$ sei die Anzahl der Zeilen und $c$ die Anzahl der Spalten von $A$.\n",
    "* Wähle die Anzahl $m$ der Merkmale, mit $m<c$. _Tipp:_ Für $m$ sollte zunächst ein Wert im Bereich $15$ bis $30$ gewählt werden.\n",
    "* Lege eine $m \\times c$ Matrix $H$ an mit initial zufälligen Elementen (Anwendung der numpy Funktion _random.random()_)\n",
    "* Lege eine $r \\times m$ Matrix $W$ an mit initial zufälligen Elementen (Anwendung der numpy Funktion _random.random()_)\n",
    "* Wiederhole bis maximale Anzahl der Iteration erreicht oder Kosten $k$ unter vordefinierter Schwelle:\n",
    "\n",
    "\t* Berechne aktuelles Produkt $B=W*H$ und bereche die Kostenfunktion \n",
    "\t\t$$\n",
    "\t\t\tk=\\left\\| A - B \\right\\|^2 = \\sum\\limits_{i,j} \\left(A_{i,j} - B_{i,j}\\right)^2\n",
    "\t\t$$ \n",
    "\t* Anpassung der Matrix $H$ durch folgende Neuberechnung der Matrixelemente\n",
    "    \n",
    "\t\t$$\n",
    "\t\tH_{i,j} := H_{i,j} \\frac{(W^T*A)_{i,j}}{(W^T*W*H)_{i,j}}\n",
    "\t\t$$\n",
    "        \n",
    "\t* __Nach__ der Anpassung der Matrix $H$: Anpassung der Matrix $W$ durch folgende Neuberechnung der Matrixelemente\n",
    "    \n",
    "\t\t$$\n",
    "\t\tW_{l,i} := W_{l,i} \\frac{(A*H^T)_{l,i}}{(W*H*H^T)_{l,i}}\n",
    "\t\t$$\n",
    "\n",
    "In [Lee, Algortihms for Non-negative Matrix Factorisation](http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf) ist bewiesen, dass durch die o.g. Anpassungsroutinen die Kosten $k$ monoton abnehmen und in einem Minimum konvergieren. Der Algorithmus ist jedoch nicht optimal weil das gefundene Minimum ein lokales Minimum sein kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Vor dem Versuch zu klärende Fragen\n",
    " \n",
    " * Was versteht man unter Artikel/Wort-Matrix? Wie wird diese im aktuellen Versuch gebildet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wie multipliziert man die Matrix\n",
    "    $$\n",
    "    A= \\left( \\begin{array}{cccc}\n",
    "a_{00} & a_{01} & a_{02} & a_{03} \\\\ \n",
    "a_{10} & a_{11} & a_{12} & a_{13} \\\\ \n",
    "a_{20} & a_{21} & a_{22} & a_{23}\n",
    "\\end{array} \\right)\n",
    "    $$\n",
    "    mit dem Vektor  \n",
    "    $$\n",
    "    v=\\left( \\begin{array}{c}\n",
    "v_{0} \\\\ \n",
    "v_{1} \\\\ \n",
    "v_{2} \\\\ \n",
    "v_{3}\n",
    "\\end{array} \\right)\n",
    "    $$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Was versteht man im Kontext der NNMF unter\n",
    "    * Merkmalsmatrix\n",
    "    * Gewichtsmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wie definieren die Zeilen der Merkmalsmatrix die einzelnen Merkmale (Topics)?\n",
    "* Was definieren die Zeilen der Gewichtungsmatrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wie werden in Numpy zwei Arrays (Typ numpy.array) \n",
    "\t* im Sinne der Matrixmultiplikation miteinander multipliziert?\n",
    "\t* elementweise multipliziert?\n",
    "* Wie wird die Transponierte eines Numpy-Arrays berechnet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versuchsdurchführung\n",
    "Die in diesem Versuch einzubindenden Feeds sind in der unten stehenden Liste _feedlist_ definiert. Die aus dem vorigen Vesuch bereits bekannte Funktion _stripHTML()_ ist ebenfalls gegeben:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anlegen der Artikel/Wort-Matrix\n",
    "\n",
    "### Die Funktion _getarticlewords()_\n",
    "Schreiben Sie eine Funktion _getarticlewords()_, die folgende Elemente zurückgibt:\n",
    "\n",
    "* _allwords:_ ist ein Dictionary dessen Keys die Worte aller gesammelten Artikel sind. Der zu jedem Key gehörende Wert ist die Anzahl, wie oft das Wort insgesamt vorkommt.\n",
    "* _articlewords:_ ist eine Liste mit so vielen Elementen wie Artikel in der Sammlung sind. Jedes Listenelement ist ein Dictionary, welches die Worte des jeweiligen Artikels als Key enthält und als Wert die Worthäufigkeit.\n",
    "* _articletitles_ ist eine Liste mit so vielen Elementen wie Artikel in der Sammlung sind. Jedes Element ist der Artikeltitel als String.\n",
    "\n",
    "Für das Parsing der Feeds soll wieder das Modul _feedparser_ eingesetzt werden. Die zu einer Nachricht gehörenden Wörter sollen die Wörter des Elements _title_ und die Wörter des Elements _description_ sein (siehe voriger Versuch). Allerdings sollen hier nicht alle Wörter eingebunden werden, sondern wie im vorigen Versuch eine Methode _getwords()_ implementiert werden, welche nur die _relevanten_ Wörter zurückgibt. Die Frage welche Wörter relevant sind ist nicht eindeutig beantwortbar. Sie können sich hierzu eigene Antworten einfallen lassen. Auf jeden Fall sollten aber die Stopwörter ignoriert werden. Hierzu kann z.B. die Stopwortliste von NLTK angewandt werden.\n",
    "\n",
    "Nachdem alle relevanten Wörter aller Nachrichten gesammelt sind, sollte eine weitere Bereinigung stattfinden, die \n",
    "\n",
    "* alle Wörter, die weniger als 4 mal vorkommen\n",
    "* alle Wörter, die in mehr als 30% aller Dokumente vorkommen\n",
    "\n",
    "entfernt. \n",
    "\n",
    "Durch dieses Herausfiltern nicht relevanter Wörter kann es vorkommen, dass einzelne Artikel keine relevanten Wörter mehr enthalten. Diese Artikel sollen dann ganz ignoriert werden. D.h. unter anderem, dass diese Artikel auch nicht in _articlewords_ und _articletitles_ erscheinen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import feedparser\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def stripHTML(h):\n",
    "  p=''\n",
    "  s=0\n",
    "  for c in h:\n",
    "    if c=='<': s=1\n",
    "    elif c=='>':\n",
    "      s=0\n",
    "      p+=' '\n",
    "    elif s==0: p+=c\n",
    "  return p\n",
    "\n",
    "\n",
    "def getArticlesFromRss():\n",
    "    feedlist=['http://feeds.reuters.com/reuters/topNews',\n",
    "          'http://feeds.reuters.com/reuters/businessNews',\n",
    "          'http://feeds.reuters.com/reuters/worldNews',\n",
    "          'http://feeds2.feedburner.com/time/world',\n",
    "          'http://feeds2.feedburner.com/time/business',\n",
    "          'http://feeds2.feedburner.com/time/politics',\n",
    "          'http://rss.cnn.com/rss/edition.rss',\n",
    "          'http://rss.cnn.com/rss/edition_world.rss',\n",
    "          'http://www.nytimes.com/services/xml/rss/nyt/GlobalHome.xml',\n",
    "          'http://feeds.nytimes.com/nyt/rss/Business',\n",
    "          'http://www.nytimes.com/services/xml/rss/nyt/World.xml',\n",
    "          'http://www.nytimes.com/services/xml/rss/nyt/Economy.xml'\n",
    "          ]\n",
    "    articles = []\n",
    "    for feed in feedlist:\n",
    "        f=feedparser.parse(feed)\n",
    "        for e in f.entries: \n",
    "            if \"description\" not in e:\n",
    "                continue\n",
    "            fulltext=stripHTML(e.title+' '+e.description)\n",
    "            articles.append(fulltext)\n",
    "\n",
    "\n",
    "    print \"Count Articles: \", len(articles)\n",
    "    return articles\n",
    "\n",
    "def putWordsIntoDict(words):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop.add(\"(reuters)\")\n",
    "    stop.add(\"new\")\n",
    "    stop.add(\"-\")\n",
    "    \n",
    "    out = dict()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in stop:\n",
    "            if word not in out:\n",
    "                out.setdefault(word, 1)\n",
    "            else:\n",
    "                out[word] += 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def getAllWords(article):\n",
    "   \n",
    "    words = []\n",
    "    out = []\n",
    "    for art in article:\n",
    "        words.extend(art.split())\n",
    "        \n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        out.append(word)\n",
    "        \n",
    "   \n",
    "    return putWordsIntoDict(out)\n",
    "\n",
    "def getArticleWordsDict(articles):\n",
    "    articlewords = []\n",
    "\n",
    "    for articel in articles:\n",
    "        a = articel.split()\n",
    "        articlewords.append(putWordsIntoDict(a))\n",
    "\n",
    "    return articlewords\n",
    "\n",
    "def getTitel(articles):\n",
    "    titel =[]\n",
    "    for article in articles:\n",
    "        head =[]\n",
    "        head.extend(article.split(\"-\"))\n",
    "        titel.append(head[0])\n",
    "    return titel\n",
    "\n",
    "def sortAndPrintDict(mydict):\n",
    "    for key, value in sorted(mydict.iteritems(), key=lambda (k,v): (v,k), reverse=True):\n",
    "        print \"%s: %s\" % (key, value)\n",
    "\n",
    "        \n",
    "def getWordsToDelete(inDict, minCount, maxCount):\n",
    "    toDel = []\n",
    "    for word in inDict:\n",
    "        if inDict[word] < minCount or inDict[word] > maxCount:\n",
    "            toDel.append(word)\n",
    "    return toDel\n",
    "\n",
    "def chooseWordsFromDict(inDict, toDel):\n",
    "    cp_inDict = dict(inDict)\n",
    "    for word in toDel:\n",
    "        if word in cp_inDict:\n",
    "            del cp_inDict[word]\n",
    "\n",
    "    return cp_inDict\n",
    "\n",
    "def chooseArticleWords(inList,toDel1):\n",
    "    cp_inList = inList[:]\n",
    "    for i in range(len(cp_inList)):\n",
    "        cp_inList[i] = chooseWordsFromDict(cp_inList[i], toDel1)\n",
    "        \n",
    "    return cp_inList\n",
    "\n",
    "def getWordList(in_allwords):\n",
    "    out_list = []\n",
    "    for o in in_allwords.iterkeys():\n",
    "        out_list.append(o)\n",
    "    return out_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'raw' not in globals():\n",
    "    raw = getArticlesFromRss()\n",
    "\n",
    "allwords = getAllWords(raw)\n",
    "chosenWords = chooseWordsFromDict(allwords, getWordsToDelete(allwords, 3, 70))\n",
    "articlewords = getArticleWordsDict(raw)\n",
    "chosenArticleWords = chooseArticleWords(articlewords, getWordsToDelete(allwords, 3, 70))\n",
    "articletitles = getTitel(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125\n"
     ]
    }
   ],
   "source": [
    "print len(allwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\n",
      "28\n",
      "264\n"
     ]
    }
   ],
   "source": [
    "print len(chosenWords)\n",
    "print len(articlewords[1])\n",
    "print len(chosenArticleWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Die Funktion _makematrix()_\n",
    "Schreiben Sie eine Funktion _makematrix()_, die aus dem Dictionary _allwords_ und der Liste _articlewords_ (vorige Aufgabe) die Artikel-/Wort-Matrix generiert. Die Einträge in der Matrix sollen die Häufigkeiten der Wörter im jeweiligen Dokument sein (term frequency tf). Die Artikel-/Wort-Matrix soll als 2-dimensionales Numpy Array angelegt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noch nicht Fertig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def makematrix(in_words, in_articlewords):\n",
    "    artwordma = np.zeros(( len(in_words) ,len(in_articlewords) ))\n",
    "    wordLi = getWordList(in_words)\n",
    "    \n",
    "    for i in range(len(in_articlewords)):\n",
    "        for j in range(len(wordLi)):\n",
    "            if wordLi[j] in in_articlewords[i]:\n",
    "                artwordma[j,i] = in_articlewords[i][wordLi[j]]\n",
    "                \n",
    "    return artwordma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print makematrix(chosenWords,chosenArticleWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Nicht Negative Matrix Faktorisierung\n",
    "Die Implementierung der NNMF ist entsprechend der Beschreibung im Theoriekapitel durchzuführen.\n",
    "\n",
    "* Implementieren Sie die Funktion _cost(A,B)_. Dieser Funktion werden zwei Numpy-Matrizen $A$ und $B$ übergeben. Zurück geliefert werden die nach oben angegebener Formel berechneten Kosten $k$. Diese Funktion wird von der im folgenden beschriebenen Funktion _nnmf(A,m,it)_ benutzt.\n",
    "* Implementieren Sie die Funktion __nnmf(A,m,it)__. In dieser Funktion soll der oben beschriebene Algorithmus für die Nicht-negative Matrix Faktorisierung ausgeführt werden. Der Funktion wird die zu faktorisierende Matrix $A$, die Anzahl der Merkmale $m$ und die Anzahl der Iterationen $it$ übergeben. Die Funktion gibt die gefundenen Faktoren $W$ und $H$ zurück. In jeder Iteration sollen mit der Funktion __cost(A,B)__ die Kosten berechnet werden. Sobald die Kostenabnahme pro 10 Iterationen kleiner als $2$ ist oder eine maximale Anzahl von Iterationen ($maxIt=200$) erreicht ist, soll der Algorithmus mit der Rückgabe der Faktoren $W$ und $H$ terminieren.     \n",
    "\n",
    "\n",
    "Tipp für die Implementierung elementweiser Operationen von Matrizen: Für elementweise Operationen müssen in Python/Numpy nicht alle Elemente über Schleifen explizit berechnet werden. Eine elementweise Anpassung aller Matrixelemente kann kompakt programmiert werden indem die beteiligten Matrizen für diese Operationen als Arrays implementiert werden. Sollen z.B. die beiden gleich großen Numpy Arrays $U$ und $V$ elementweise multipliziert werden, dann wäre der entsprechende Programmcode einfach _U*V_.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anzeige der Merkmale und der Gewichte\n",
    "\n",
    "Im vorigen Abschnitt wurde die Merkmalsmatrix $H$ und die Gewichtsmatrix $W$ berechnet. Diese Matrizen können natürlich am Bildschirm ausgegeben werden, was jedoch nicht besonders informativ ist. Aus den Matrizen können jedoch die Antworten für die folgenden interessanten Fragen berechnet werden:\n",
    "\n",
    "* In welchen Artikeln sind welche Merkmale stark vertreten?\n",
    "* Wie lassen sich die insgesamt $m$ Merkmale beschreiben, so dass aus dieser Merkmalsbeschreibung klar wird, welches Thema den Artikeln, in denen das Merkmal stark vertreten ist, behandelt wird? \n",
    " \n",
    "Die Antwort auf die erste Frage ergibt sich aus der Gewichtsmatrix $W$. Für die Beantwortung der zweiten Frage wird die Merkmalsmatrix $H$ herangezogen.\n",
    "\n",
    "\n",
    "\n",
    "### Beschreibung der Merkmale\n",
    "\n",
    "Die Merkmalsmatrix $H$ beschreibt, wie stark die Worte aus _wordvec_ in jedem Merkmal enthalten sind. Jede Zeile von $H$ gehört zu einem Merkmal, jede Spalte von $H$ gehört zu einem Wort in _wordvec_.\n",
    "\n",
    "Es bietet sich an jedes Merkmal einfach durch die $N=6$ Wörter aus _wordvec_ zu beschreiben, welche am stärksten in diesem Merkmal enthalten sind. Hierzu muss für jedes Merkmal die entsprechende Zeile in $H$ nach den $N=6$ größten Werten durchsucht bzw. geordnet werden. Die entsprechenden Spalten dieser Matrixelemente verweisen dann auf die $N=6$ wichtigsten Worte des Merkmals.\n",
    "\n",
    "Tipp für die Implementierung: Legen Sie für jedes Merkmal $i$ eine Liste an. Die Listenlänge ist durch die Anzahl der Worte in _wordvec_ (d.h. die Anzahl der Spalten in $H$) gegeben. Jedes Listenelement $j$ enthält selbst wieder 2 Elemente: An erster Stelle den entsprechenden Wert $H_{i,j}$ der Merkmalsmatrix, an der zweiten Stelle das $j.$-te Wort in _wordvec_. Nachdem die Liste angelegt ist, kann sie mit _listname.sort()_ in aufsteigender Reihenfolge sortiert werden. Die abnehmende Sortierung erhält man mit _listname.sort().reverse()_. Danach geben die $N=6$ ersten Listenelemente die für das Merkmal $i$ wichtigsten Worte an.\n",
    "\n",
    "   \n",
    "### Präsenz der Merkmale in den Artikeln\n",
    "\n",
    "Die Gewichtsmatrix $W$ beschreibt, wie stark die $m$ Merkmale in den Artikeln aus _articletitles_ enthalten sind. Jede Zeile von $W$ gehört zu einem Artikel, jede Spalte von $W$ gehört zu einem Merkmal.\n",
    "Die Berechnung der $M=2$ gewichtigsten Merkmale für jeden Artikel in _articletitles_ kann analog zu der oben beschriebenen Berechnung der $N=6$ wichtigsten Worte eines Merkmals berechnet werden.\n",
    "\n",
    "\n",
    "### Implementierung\n",
    "\n",
    "Implementieren Sie eine Funktion _showfeatures(w,h,titles,wordvec)_, welche wie oben beschrieben für jeden Artikel die $M=2$ wichtigsten Merkmale am Bildschirm ausgibt. Dabei soll jedes Merkmal durch die 6 wichtigsten Wörter dieses Merkmals angegeben werden. Siehe Beispielausgabe unten.  \n",
    "\n",
    "Übergabeparameter der Funktion sind die Merkmalsmatrix $H$, die Gewichtungsmatrix $W$, die Liste aller Artikeltitel _articletitles_ und die Liste aller Worte _wordvec_.\n",
    "\n",
    "\n",
    "Beispiel fuer Ausgabe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[(13.54131155883748, 13, u'Putin vows payback after confirmation of Egypt plane bomb'),\n",
    "\n",
    "(2.2466669548146254, 9, u'Putin vows payback after confirmation of Egypt plane bomb')]\n",
    "\n",
    "----- ['plane', 'egypt', 'russia', 'month', 'killing', 'putin']\n",
    "\n",
    "----- ['airport', 'russian', 'crash', 'egypt', 'security', 'officials']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ausgabe ist wie folgt zu interpretieren:\n",
    "* Für den Artikel _Putin vows payback after confirmation of Egypt plane bomb_ ist \n",
    "    * das wichtigste Merkmal durch die 6 Wörter _plane_, _egypt_, _russia_, _month_, _killing_, _putin_ definiert. Das Gewicht dieses Merkmals im Artikel ist 13.54\n",
    "    * das zweitwichtigste Merkmal durch die 6 Wörter _airport_, _russian_, _crash_, _egypt_, _security_, _officials_ definiert. Das Gewicht dieses Merkmals im Artikel ist 2.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgaben\n",
    "\n",
    "1. Analysieren Sie die berechneten Topics indem Sie sich überlegen ob die gefundenen 6 Wörter pro Topic wirklich Themen beschreiben.\n",
    "2. Verändern Sie die Parameter der NNMF (Anzahl der Topics $m$, Anzahl der Iterationen). Bei welcher Einstellung der Parameter erhalten Sie das für sie sinnvollste Resultat (sinnvolle Topics)?\n",
    "3. Wie kann die _getwords()_ Methode verbessert werden, so dass noch bedeutsamere Topics gefunden werden? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
